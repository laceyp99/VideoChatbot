{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video QA GPT\n",
    "* Using OpenAI's vision models to understand video and answer any questions about the video.\n",
    "\n",
    "Idea for implementation:\n",
    "\n",
    "* resized to 150x150, but can be changed easily (cost adjustment)\n",
    "* 250 frame API limit on input / 30 fps = 8.3 seconds of video\n",
    "    * These 250 frames can skip frames, models don't see video data the same way humans do. \n",
    "    * 2x = 16.6 seconds, 4x = 33.2 seconds\n",
    "\n",
    "## Approach\n",
    "I made it so that every 250 frames (about 8.3 seconds at 30 fps), I would prompt GPT 4o-mini to describe the video in detail alongside a frame recomendation so that a single frame from the 250 can be included in context on the final generation (that might not be necessary). I also made it so that the frames skip by 4 for faster/cheaper processing. This makes it so that the video length isn't as limited.\n",
    "\n",
    "## Cost Assesment\n",
    "\n",
    "**GPT-4o**:\n",
    "Using this video processing method is pretty expensive. Even at 150x150, it costs $0.32 for one set of 250 frames. Then incorporate the additional system message text and the expected output text (output tokens cost ~3x more than input normally). Thats $0.35-0.50 every 8.3 seconds of video. Thats $2.50-$3.50 every minute of video. Also this is just to generate the helpful context to supply a conversational chatbot that can answer questions about the video (which would then add more cost), so this is all a video processing fee before the user even can try out the chatbot. That seems pretty unrealistic. I'd never pay $2.50+ to have a chatbot answer questions on a one minute video.\n",
    "\n",
    "**GPT-4o-mini**:\n",
    "GPT 4-o mini is way more cost efficient. A little bit less than a penny for every 250 frames (8.3 seconds) of video. There will still be an uploading cost attached to this ($0.32 for almost 5 mins), but thats more reasonable. Also compressing the video by 4x frames helps with processing cost and time. \n",
    "\n",
    "## Time Assesment \n",
    "One generation with 250 frames + text prompt can take up to 2-3 minutes for some reason. Making all the description generations async so that it runs in parallel instead of series. This will save you n-fold on video processing time. \n",
    "\n",
    "### Work on:\n",
    "* Make description generation function async for parallel processing\n",
    "* Create an option to change from 1,2 and 4x \"frame speed\"\n",
    "* Do I need the frame recommendation? The last generation will have a lot of text and it should be encompassing (test out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai python-dotenv opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "import cv2  # We're using OpenCV to read video, to install !pip install opencv-python\n",
    "import base64\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os, json, sys\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set API Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('.env')\n",
    "# print(os.getenv('OPENAI_API_KEY'))\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "# print(client.chat.completions.create(model='gpt-4o-mini', messages=[{'role': 'user', 'content': 'What is the capital of France?'}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get video path and initialize global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your video path here\n",
    "video_path = 'enter_your_video_path_here.mp4'\n",
    "\n",
    "CONTEXT_LIMIT = 128000 # 128k tokens for GPT 4-v and GPT 4-o(mini)\n",
    "MAX_FRAMES = 250 # OpenAI API limit for video frames\n",
    "SIZE_LIMIT_MB = 20 # OpenAI API limit 20MB for image size\n",
    "GPT_4o_MINI_MAX_GEN_TOKENS = 16384 # 16k tokens for GPT 4-o(mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "### Resize and encode video\n",
    "* We can resize the video to 150x150 (we can change the resize dimensions) for cost effectiveness.\n",
    "    * The bigger the video dimension, the more tokens per frame\n",
    "* We encode the video to base64 encoding\n",
    "    * We decode the video frame by frame as a .jpg image\n",
    "    * Then we decode the base64 encoded data into utf-8 for openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_encode_video(input_path, width, height):\n",
    "    # Open the input video\n",
    "    input_video = cv2.VideoCapture(input_path)\n",
    "    if not input_video.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return []\n",
    "\n",
    "    base64Frames = []\n",
    "    while True:\n",
    "        ret, frame = input_video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Resize the frame\n",
    "        resized_frame = cv2.resize(frame, (width, height))\n",
    "\n",
    "        # Encode the frame to JPEG\n",
    "        _, buffer = cv2.imencode(\".jpg\", resized_frame)\n",
    "        base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
    "\n",
    "    # Get FPS\n",
    "    fps = input_video.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Release the video capture\n",
    "    input_video.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    print(len(base64Frames), \"frames read.\")\n",
    "    return base64Frames, fps\n",
    "\n",
    "# Example usage\n",
    "base64Frames, fps = resize_and_encode_video(video_path, 150, 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the video got resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_dimensions(frames_array):\n",
    "    # Check if video opened successfully\n",
    "    if frames_array:\n",
    "        # Decode base64 to bytes\n",
    "        frame_data = base64.b64decode(frames_array[0])\n",
    "        \n",
    "        # Convert bytes data to numpy array\n",
    "        nparr = np.frombuffer(frame_data, np.uint8)\n",
    "        \n",
    "        # Decode image\n",
    "        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        # Get dimensions\n",
    "        height, width = img.shape[:2]\n",
    "        print(\"Video dimensions:\", width, \"x\", height)\n",
    "    else:\n",
    "        print(\"The list of frames is empty.\")\n",
    "        height, width = 0, 0\n",
    "    return height, width\n",
    "\n",
    "\n",
    "height, width = get_video_dimensions(base64Frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take out excess frames\n",
    "* Saves on processing time and cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define how we are going to compress the video frames\n",
    "# We can simply take every N-th frame, that way we compress the video while keeping the same length\n",
    "def compress_video_frames(frames_array, n):\n",
    "    return frames_array[::n]\n",
    "\n",
    "base64Frames = compress_video_frames(base64Frames, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the video that will be shown to the model\n",
    "* Remember the model is able to use each frame as context information\n",
    "    * Different than how humans experience video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_handle = display(None, display_id=True)\n",
    "for img in base64Frames: \n",
    "    display_handle.update(Image(data=base64.b64decode(img.encode(\"utf-8\"))))\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the tokens for each frame (image)\n",
    "* By this time, the video got resized to 150x150, so this function will consistently output the same number of tokens\n",
    "    * This function makes the code modular in the way that you can change the resize dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "def calculate_image_tokens(width: int, height: int):\n",
    "    if width > 2048 or height > 2048:\n",
    "        aspect_ratio = width / height\n",
    "        if aspect_ratio > 1:\n",
    "            width, height = 2048, int(2048 / aspect_ratio)\n",
    "        else:\n",
    "            width, height = int(2048 * aspect_ratio), 2048\n",
    "            \n",
    "    if width >= height and height > 768:\n",
    "        width, height = int((768 / height) * width), 768\n",
    "    elif height > width and width > 768:\n",
    "        width, height = 768, int((768 / width) * height)\n",
    "\n",
    "    tiles_width = ceil(width / 512)\n",
    "    tiles_height = ceil(height / 512)\n",
    "    total_tokens = 85 + 170 * (tiles_width * tiles_height)\n",
    "    \n",
    "    return total_tokens\n",
    "\n",
    "tokens_per_frame = calculate_image_tokens(width, height)\n",
    "print(\"Tokens per frame:\", tokens_per_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of tokens for a video with 250 frames (OpenAI API limit)\n",
    "video_tokens = MAX_FRAMES * tokens_per_frame\n",
    "\n",
    "# Initialize the cost variables for gpt-4o and gpt-4o-mini\n",
    "input_token_cost_per1k = 0.0050\n",
    "output_token_cost_per1k = 0.0150\n",
    "mini_input_token_cost_per1k = 0.00015\n",
    "mini_output_token_cost_per1k = 0.0006\n",
    "\n",
    "# Calculate the cost for 250 frames\n",
    "cost_frames_250 = (video_tokens/1000) * mini_input_token_cost_per1k\n",
    "\n",
    "# Calculate the maximum tokens and cost for the natural language description output\n",
    "# max_output_tokens = (CONTEXT_LIMIT - video_tokens) - 250 # padding for prompt and other tokens\n",
    "max_output_cost = GPT_4o_MINI_MAX_GEN_TOKENS / 1000 * mini_output_token_cost_per1k\n",
    "\n",
    "# Calculate how many generations are needed for the preprocessed video\n",
    "v_length_frames = len(base64Frames)\n",
    "if (v_length_frames % MAX_FRAMES) == 0:\n",
    "    num_generations = v_length_frames // MAX_FRAMES\n",
    "else:\n",
    "    num_generations = v_length_frames // MAX_FRAMES + 1\n",
    "\n",
    "# Calculate the cost of the uploaded video and the video description generations\n",
    "input_video_cost = num_generations * cost_frames_250\n",
    "max_desc_generation_cost = num_generations * max_output_cost\n",
    "\n",
    "# Calculate the total cost of video processing\n",
    "max_total_video_processing_cost = input_video_cost + max_desc_generation_cost\n",
    "\n",
    "# Print the results\n",
    "print(\"Total tokens for 250 frames:\", video_tokens)\n",
    "print(f\"Total cost for 250 frames: ${cost_frames_250}\")\n",
    "print(f\"Max output cost: ${max_output_cost}\\n\") # max_output_cost\n",
    "print(f\"Your video needs {num_generations} descriptions.\")\n",
    "print(f\"Total cost of uploaded video: ${input_video_cost}\")\n",
    "print(f\"Total cost of video description generations: ${max_desc_generation_cost}\")\n",
    "print(f\"Total cost of video processing: ${max_total_video_processing_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Video Chunk Description\n",
    "* Added try/except block for API call due to various possible errors.\n",
    "    * This allows the program fill out the JSON even if one API call results in an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_video_description(frames_array):\n",
    "    \"\"\"\n",
    "    This function generates a description of the video for every 250 frames.\n",
    "    \"\"\"\n",
    "    total_frames = len(frames_array)\n",
    "    descriptions = []\n",
    "    counter = 1\n",
    "    for i in range(0, total_frames, MAX_FRAMES):\n",
    "        end_frame = min(i + MAX_FRAMES, total_frames)\n",
    "        video_chunk = frames_array[i:end_frame]\n",
    "        chunk_size = sum(sys.getsizeof(frame) for frame in video_chunk)\n",
    "        chunk_size_mb = chunk_size / (1024 * 1024)\n",
    "        print(f\"Generating description of chunk {counter} from frame {i} to {end_frame}\\nChunk size: {chunk_size_mb:.2f} MB\")\n",
    "        try:\n",
    "            description = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a helpful video description assistant. Please describe the video that the user inputs with as much detail as possible. Be specific about colors, numbers, and all the fine details of the scene. Please format the description as a paragraph (no bullet points, or numbering) When you're done, please pick the frame number (1-250) that you think would best represent the video (ex. Frame Recomendation: 125).\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            \"Describe the video\", *map(lambda x: {\"image\": x}, video_chunk)\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=GPT_4o_MINI_MAX_GEN_TOKENS,\n",
    "            )\n",
    "            generation_desc = description.choices[0].message.content\n",
    "            print(generation_desc)\n",
    "            chunk_dict = {}\n",
    "            chunk_dict[\"start_frame\"] = i\n",
    "            chunk_dict[\"end_frame\"] = end_frame\n",
    "            chunk_dict[\"total_tokens\"] = description.usage.total_tokens\n",
    "            if (generation_desc.rfind(\"Frame Recomendation: \") != -1):\n",
    "                index = generation_desc.rfind(\"Frame Recomendation: \")\n",
    "                frame_number = int(generation_desc[index:3].strip())\n",
    "                print(\"Frame Recomendation: \", frame_number, \"\\n\\n\")\n",
    "                chunk_dict[\"frame_number\"] = frame_number\n",
    "                chunk_dict[\"frame_data\"] = frames_array[i + frame_number - 1]\n",
    "            else:\n",
    "                print(\"No frame recommendation found. Defaulting to frame 125.\\n\\n\")\n",
    "                chunk_dict[\"frame_number\"] = 125\n",
    "                chunk_dict[\"frame_data\"] = frames_array[125]\n",
    "            chunk_dict[\"description\"] = generation_desc\n",
    "        except Exception as e:\n",
    "            print(\"Error generating description:\", str(e))\n",
    "            chunk_dict = {}\n",
    "            chunk_dict[\"start_frame\"] = i\n",
    "            chunk_dict[\"end_frame\"] = end_frame\n",
    "            chunk_dict[\"description\"] = \"Error generating description\"\n",
    "            chunk_dict[\"total_tokens\"] = 0\n",
    "            chunk_dict[\"frame_number\"] = 125\n",
    "            chunk_dict[\"frame_data\"] = frames_array[125]\n",
    "\n",
    "        descriptions.append(chunk_dict)\n",
    "        counter += 1\n",
    "    return descriptions\n",
    "\n",
    "video_descriptions = generate_video_description(base64Frames)\n",
    "# Save the video descriptions to a file\n",
    "with open(\"video_descriptions.json\", \"w\") as f:\n",
    "    json.dump(video_descriptions, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a peek at the description data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_descriptions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the API Call with video description context\n",
    "* message_list as an argument for the conversational history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using video_descriptions as a video summary, answer questions about the video\n",
    "\n",
    "def ask_informed_model(message_list):\n",
    "    ''' \n",
    "    This function prompts a GPT-4o model with a question and a video description.\n",
    "    '''\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = message_list,\n",
    "        temperature=0,\n",
    "    )\n",
    "    # print(response.usage.total_tokens)\n",
    "    response_text = response.choices[0].message.content\n",
    "    message_list.append(\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": response_text\n",
    "        }\n",
    "    )\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_description = \"\"\n",
    "for set in video_descriptions:\n",
    "    total_description += set[\"description\"] + \" \"\n",
    "msgs = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"You are a helpful video assistant. Please answer the following questions about the video in which you have the text description and a reference frame right here: {total_description}. If you aren't sure of the answer, you can say 'I am not sure. Heres what I have [unsure answer], and here is what I am basing it off of [knowledge]'.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "while True:\n",
    "    prompt = input(\"Ask a question about the video (enter 'exit' to stop): \").strip()\n",
    "    if prompt.lower() == \"exit\":\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"User: {prompt}\")\n",
    "        msgs.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        )\n",
    "    print(f\"Assistant: {ask_informed_model(msgs)}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
